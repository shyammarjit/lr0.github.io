<!DOCTYPE html>
<html>
<head>
  <script type="text/javascript" async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>

  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LR0.FM: Low-Resolution Zero-shot Classification Benchmark For Foundation Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
<!-- 
  <style>
    .gif1 {
        width: 450px; /* Adjust width as needed */
        height: 275px; /* Adjust width as needed */
    }
    .gif2 {
        width: 555px; /* Adjust width as needed */
        height: 275px; /* Adjust width as needed */
    }
  </style> -->
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="font-size: 42px;">LR0.FM: Low-Resolution Zero-shot Classification Benchmark For Foundation Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ppriyank.github.io/" target="_blank">Priyank Pathak</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/shyammarjit" target="_blank">Shyam Marjit</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://mse.ucf.edu/person/shrutivyas/" target="_blank">Shruti Vyas</a><sup>1</sup>, &
            </span>
            <span class="author-block">
              <a href="https://www.crcv.ucf.edu/person/rawat/" target="_blank">Yogesh S Rawat</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Central Florida,</span>
            <span class="author-block"><sup>2</sup>IIIT Guwahati</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://iclr.cc/virtual/2025/poster/30609" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.03950" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/shyammarjit/LR0.FM" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->

<section class="hero teaser">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <!-- <div id="results-carousel" class="carousel results-carousel"> -->
              
              <div class="item"> <!-- Your image here -->
                  <img src="static/images/acc_drop.png" width="100%" style="display: block; margin-left: auto; margin-right: auto; margin-bottom: 8px; margin-top: auto;" />
              </div>
              <p class="carousel-caption" style="width: 82%; margin-left: auto; margin-right: auto; text-align: center;">
                <strong> Fall of Top-1 zero-shot classification accuracy (y-axis) with resolution (x-axis):</strong>  Backbones for foundation models are merged as shade, with average performance across backbones in the dark.
              </p>
              
              <div class="item"> <!-- Your image here -->
                  <img src="static/images/low_res.png" width="100%" style="display: block; margin-left: auto; margin-right: auto; margin-bottom: 8px; margin-top: 25px" />
              </div>
              <p class="carousel-caption" style="width: 90%; margin-left: auto; margin-right: auto; text-align: center;">
                <strong> Zero-Shot Semantically correct misclassifications:</strong> EVA-CLIP correct classification at 224×224 (<span style="color: rgb(4, 173, 101);">green</span>) & misclassification at lower resolution (<span style="color: rgb(254,1, 2);">red</span>). However, ImageNet labels-based mispredictions are semantically reasonable (humans), 
                  <span style="display: block; text-align: center;">
                      indicating viability of pre-trained weights at low resolution.
                  </span>
              </p>

          <!-- </div> -->
      </div>
  </div>
</section>


<!-- Abstract. -->
<hr class="divider" />
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths" style="margin-top: -25px; margin-bottom: -20px">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Visual-language foundation Models (FMs) exhibit remarkable zero-shot generalization across diverse tasks, 
            largely attributed to extensive pre-training on largescale datasets. 
            However, their robustness on low-resolution/pixelated (LR) images, a common challenge in real-world scenarios, 
            remains underexplored.
          </p>
          
          <p>
            We introduce <b>LR0.FM</b>, a comprehensive benchmark evaluating the impact of low resolution on the zero-shot 
            classification performance of 10 FM(s) across 66 backbones and 15 datasets.
            We propose a novel metric, <b>Weighted Aggregated Robustness</b>, to address the limitations of existing metrics 
            and better evaluate model performance across resolutions and datasets.
          </p> 
                            
          <p>
            Our key findings show that: 
            (i) model size positively correlates with robustness to resolution degradation,
            (ii) pre-training dataset quality is more important than its size, and 
            (iii) fine-tuned and higher resolution models are less robust against LR.
          </p>

          <p>
            Our analysis further reveals that 
            the model makes semantically reasonable predictions at LR, and the lack of fine-grained details in input 
            adversely impacts the model’s initial layers more than the deeper layers. 
            We use these insights and introduce a simple strategy, <b>LR-TK0</b>, to enhance the robustness of models without 
            compromising their pre-trained weights. We demonstrate the effectiveness of <b>LR-TK0</b> for robustness against 
            low-resolution across several datasets and its generalization capability across backbones and other approaches.
          </p>

        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<!-- Weighted Aggregated Robustness -->
<hr class="divider" />
<section class="section hero">
  <div class="container is-max-desktop">
      <div class="section-title has-text-centered is-centered">
          <h2 class="title is-3">🚀 Weighted Aggregated Robustness</h2>
          <div class="content has-text-justified">
            <p>
              <b><i>Problem A) Misleading high robustness:</i> </b> : If the model performs poorly on a challenging dataset
              i.e. performance close to random predictions, then downsampling will likely maintain this random
              prediction with minimal drop in accuracy, giving abnormally high robustness score.
            </p>

            <p>
              <b><i>▷ Solution: Improved Relative Robustness:</i> </b>   We
              propose zeroing out robustness near random predictions. 
              <div style="display: flex; justify-content: center; align-items: center; gap: 100px;">
                <img src="static/images/improved robustness.jpeg" width="50%" height="50%">
                <img src="static/images/new_alpha_page-0001.jpg" width="30%" height="30%">
              </div>

            </p>

            <p>
              <b><i>Problem B) SAR overlooks datasets:</i> </b> When comparing models, their robustness scores are averaged across datasets (giving each dataset a score of 1, SAR). Ideally, the model rankings, after averaging, should stay consistent with
              individual dataset rankings. Model rankings on datasets like ImageNet overshadows the ranking of datasets like
              ImageNet-A and EuroSAT, which behave differently. This makes the final comparison exclude such datasests, as if these datasets aren’t present (<span style="color: rgb(254,1, 2);">left, below</span>). 
            </p>

            <p>
              <b><i>▷ Solution: Weighted Aggregated Robustness</i> </b>: We propose adjusting the dataset weights so that the model rankings after aggregation
              reflect each dataset fairly. Weights are optimized such that the correlation (Spearman) between the model rankings after the weighted average and individual dataset rankings are maxi-
              mized. (<span style="color: rgb(4, 173, 101);">right, below</span>)
            </p>

          <div class="columns is-centered">
            <div class="column">
              <div class="content">
                <!-- <h2 class="title is-3">LR Tokens</h2> -->
                <img src="static/images/Slide7.jpeg" width="100%" height="100%">
              </div>
            </div>

            <div class="column">
              <div class="content">
                <!-- <h2 class="title is-3">LR Tokens</h2> -->
                <img src="static/images/Slide8.jpeg" width="100%" height="100%">
              </div>
            </div>
          </div>

      </div>
  </div>
</section>

<!-- LR Tokens and LR-TK0 -->
<hr class="divider" />
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="section-title has-text-centered is-centered"></div>
    <h2 class="title is-3" style="display: flex; justify-content: center; align-items: center;">👻 Proposed Technique</h2>
    <div class="content has-text-justified">
      <p>
        Our technique freezes the existing model weights and trains the trainable tokens "LR Tokens" (left) via the self-supervised technique "LR-TK0" (right) on synthetic dataset with any annotations and labels. 
      </p>

    <div class="columns is-centered">
      <!-- LR tokens. -->
      <div class="column">
        <div class="content">
          <h3 class="title is-3">LR Tokens</h3>
          <p>
            ▷ LR tokens are added to the frozen spatial patches (white) after patch generation, before each frozen transformer block, and class token as a final feature.
          </p>
          <!-- <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/images/Models.mp4"
                    type="video/mp4">
          </video> -->
          
          <div style="display: flex; justify-content: center; align-items: center;">
            <img src="static/images/Model.gif" width="80%" height="100%">
          </div>
          
        </div>
      </div>
      <!--/ LR tokens. -->

      <!-- LR-TK0. -->
      <div class="column">
        <h3 class="title is-3">LR-TK0</h3>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              ▷ LR-TK0: Multi-scale training (only 1 shown for simplicity). Teacher (w/o LR tokens) generates \( f^T_{HR} \) (HR), Student (w/ LR tokens) generates both \( f^S_{HR}, f^S_{LR}\).
            </p>
            <!-- <video id="matting-video" controls playsinline height="100%">
              <source src="./static/images/Method-SSs.mp4"
                      type="video/mp4">
            </video> -->
            <img src="static/images/Method-SSs.gif">
          </div>

        </div>
      </div>
      <!--/ LR-TK0. -->
    </div>
  </div>
</section>

<!-- Zero-Shot High Resolution Dataset Synthesis-->
<hr class="divider" />
<section class="section hero">
  <div class="container is-max-desktop">
      <div class="section-title has-text-centered is-centered">
          <h2 class="title is-3">🤖 Zero-Shot High Resolution Dataset Synthesis</h2>
          <div class="content has-text-justified">
            <p>
              We use the diffusion model PIXART-&alpha; to generate synthetic HR images, via
                7,000 randomly sampled captions from Conceptual Captions.
            </p>

          <div class="columns is-centered">
            <div class="column">
              <div class="content">
                <!-- <h2 class="title is-3">LR Tokens</h2> -->
                <div style="display: flex; justify-content: center; align-items: center;">
                  <img src="static/images/gen_images.png" width="110%" height="110%">
                </div>
                <p class="carousel-caption" style="width: 90%; margin-left: auto; margin-right: auto; text-align: center;">
                  <strong>Samples generated using the captions randomly sampled from Conceptual Captions.</strong>
                </p>
                
              </div>
            </div>

            <div class="column">
              <div class="content">
                <div style="display: flex; justify-content: center; align-items: center;">
                  <img src="static/images/gen_image_prompts.png" width="65%" height="80%">
                </div>
                
                <p class="carousel-caption" style="width: 90%; margin-left: auto; margin-right: auto; text-align: center;">
                  <strong>Multiple images per caption generated via different seeds.</strong>
                </p>
              </div>
            </div>
          </div>

      </div>
  </div>
</section>


<!-- Experimental Results --> 
<hr class="divider" />
<section class="section hero">
  <div class="container is-max-desktop">
      <div class="section-title has-text-centered is-centered">
          <h2 class="title is-3">Experimental Results</h2>
          <div class="content has-text-justified">
            <div class="content">
              <p class="carousel-caption" style="width: 90%; margin-left: auto; margin-right: auto; text-align: center; margin-bottom: -5px">
                <strong>▷ <b>LR-TK0 improvement on Foundation models:</b> EVA-B/16</strong> is EVA
                  <strong>Meta-B/16</strong>: MetaCLIP-ViT-B/16 (2.5B), <strong>OC-B/16</strong>: OpenCLIP-ViT-B/16. Higher number &prop; better performance.
              </p>

              <iframe src="table.html" width="100%" height="300px" style="border:none;"></iframe>

            </div>

            <div class="content">
              <img src="static/images/proposed_base.png" width="100%" height="100%" style="margin-top: -40px;">
              <p class="carousel-caption" style="width: 90%; margin-left: auto; margin-right: auto; text-align: center; margin-bottom: 35px;">
                Baseline vs LR-TK0: Top-1 accuracy for EVA-CLIP-B/16 on 16×16. 
                <!-- <b><i>Right:</i></b> LR-TK0 improves all EVA backbones: L@336 is L/14 with 336×336 input. -->
              </p>
            </div>

            <div style="display: flex; justify-content: center; align-items: center; gap: 200px;">
              <img src="static/images/imgs_captions_SAR.png" width="30%" height="30%">
              <img src="static/images/eva_backbones.png" width="30%" height="30%">
            </div>
              <p class="carousel-caption" style="width: 90%; margin-left: auto; margin-right: auto; text-align: center;">
                ▷ <b><i>Left:</i></b> #Images/Caption: Robustness vs. Size of diffusion generated dataset. <b><i>Right:</i></b> Baseline vs LR-TK0: Top-1 accuracy for EVA-CLIP-B/16 on 16×16. 
              </p>
            <!-- </div> -->

          </div>

      </div>
  </div>
</section>

<hr class="divider" />
<!-- for citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{
        pathak2025lrfm,
        title={{LR}0.{FM}: {LOW}-{RESOLUTION} {ZERO}-{SHOT} {CLASSIFICATION} {BENCHMARK} {FOR} {FOUNDATION} {MODELS}},
        author={Priyank Pathak and Shyam Marjit and Shruti Vyas and Yogesh S Rawat},
        booktitle={The Thirteenth International Conference on Learning Representations},
        year={2025},
        url={https://openreview.net/forum?id=AsFxRSLtqR}
        }
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align: center;">
          Page credits: <a href="https://nerfies.github.io" target="_blank">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
